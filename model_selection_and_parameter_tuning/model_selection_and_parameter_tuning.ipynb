{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In this chapter, we will be working with the Breast Cancer Wisconsin dataset, \n",
    "which contains 569 samples of malignant and benign tumor cells. \n",
    "The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), \n",
    "respectively. The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, \n",
    "which can be used to build a model to predict whether a tumor is benign or malignant. \n",
    "The Breast Cancer Wisconsin dataset has been deposited on the UCI machine learning repository and more detailed information \n",
    "about this dataset can be found at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic).\n",
    "'''\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = df.loc[:, 2:].values\n",
    "y =df.loc[:, 1].values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Files\\anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "we need to standardize the columns in the Breast Cancer Wisconsin dataset before we can feed them to a linear classifier,\n",
    "such as logistic regression. Furthermore, let's assume that we want to compress our data from the initial 30 dimensions\n",
    "onto a lower two-dimensional subspace via principal component analysis (PCA), \n",
    "a feature extraction technique for dimensionality reduction that we introduced in Chapter 5,\n",
    "Compressing Data via Dimensionality Reduction. \n",
    "Instead of going through the fitting and transformation steps for the training and test dataset separately, we can chain the StandardScaler,\n",
    "PCA, and LogisticRegression objects in a pipeline\n",
    "'''\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),('pca', PCA(n_components=2)),('clf', LogisticRegression(random_state=1))])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.949565217391\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In k-fold cross-validation, we randomly split the training dataset into k folds without replacement, \n",
    "where k-1 folds are used for the model training and one fold is used for testing. \n",
    "This procedure is repeated k times so that we obtain k models and performance estimates.\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "kfold = StratifiedKFold(y=y_train,n_folds=10,random_state=1)\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [ 0.89130435  0.97826087  0.97826087  0.91304348  0.93478261  0.97777778\n",
      "  0.93333333  0.95555556  0.97777778  0.95555556]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "scikit-learn also implements a k-fold cross-validation\n",
    "scorer, which allows us to evaluate our model using stratified k-fold\n",
    "cross-validation more efficiently:\n",
    "'''\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(estimator = pipe_lr, X=X_train,y=y_train,cv=10, n_jobs=1)\n",
    "print 'CV accuracy scores: %s' % scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Files\\anaconda\\lib\\site-packages\\sklearn\\learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Diagnosing bias and variance problems with learning curves\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),('clf', LogisticRegression(penalty='l2', random_state=0))])\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=10, n_jobs=1)\n",
    "#print train_scores, test_scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "plt.plot(train_sizes, train_mean,color='blue', marker='o',markersize=5,label='training accuracy')\n",
    "plt.plot(train_sizes, test_mean,color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Addressing overfitting and underfitting with validation curves\n",
    "Validation curves are a useful tool for improving the performance of a model by addressing issues such as overfitting or underfitting.\n",
    "Validation curves are related to learning curves, but instead of plotting the training and test accuracies as functions of the sample size,\n",
    "we vary the values of the model parameters, for example, the inverse regularization parameter C in logistic regression.\n",
    "'''\n",
    "from sklearn.learning_curve import validation_curve\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores, test_scores = validation_curve(estimator=pipe_lr, X=X_train, y=y_train, param_name='clf__C', param_range=param_range, cv=10)\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "plt.plot(param_range, train_mean,color='blue', marker='o', markersize=5, label='training accuracy')\n",
    "plt.plot(param_range, test_mean,color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978021978022\n",
      "Test accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tuning hyperparameters via grid search\n",
    "\n",
    "The approach of grid search is quite simple, \n",
    "it's a brute-force exhaustive search paradigm where we specify a list of values for different hyperparameters, \n",
    "and the computer evaluates the model performance for each combination of those to obtain the optimal set\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))])\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'clf__C': param_range, 'clf__kernel': ['linear']}, {'clf__C': param_range, 'clf__gamma': param_range, 'clf__kernel': ['rbf']}]\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print gs.best_score_\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print 'Test accuracy: %.3f' % clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.976\n",
      "Recall: 0.952\n",
      "F1: 0.964\n",
      "auc: 0.969\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Looking at different performance evaluation metrics\n",
    "\n",
    "we evaluated our models using the model accuracy, which is a useful metric to quantify the performance of a model in general.\n",
    "However, there are several other performance metrics that can be used to measure a model's relevance, such as precision, recall,\n",
    "and the F1-score\n",
    "'''\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n",
    "print('auc: %.3f' % roc_auc_score(y_true=y_test, y_score=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
